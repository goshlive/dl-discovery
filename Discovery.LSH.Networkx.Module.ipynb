{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f0d60f8",
   "metadata": {},
   "source": [
    "## Module to perform DATA DISCOVERY in DATA LAKES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266de480",
   "metadata": {},
   "source": [
    "## IMPORTS ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7bd8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Created: Aug 13, 2023\n",
    "# Updated: Sep 3, 2023 16:45\n",
    "# Location: Budapest, Hungary\n",
    "##################################\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy.linalg import cholesky\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "from discovery_lsh_netw import Node\n",
    "from discovery_lsh_netw import Edge\n",
    "from discovery_lsh_netw import create_edge_key\n",
    "from discovery_lsh_netw import create_netw\n",
    "\n",
    "import pickle\n",
    "\n",
    "from csv_schema_inference import csv_schema_inference\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "all_stopwords = stopwords.words('english')\n",
    "\n",
    "my_random = random.Random(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52921a0d",
   "metadata": {},
   "source": [
    "## CODE FROM https://doi.org/10.1007/s10994-020-05882-8 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35653b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The SCD algorithm, Skrlj and Kralj, 2019 (2020 accepted).\n",
    "\"\"\"\n",
    "import tqdm\n",
    "import scipy.sparse as sp\n",
    "import scipy as spy\n",
    "from scipy.sparse import csgraph\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from itertools import product\n",
    "import operator\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%d-%b-%y %H:%M:%S')\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class SCD_obj:\n",
    "    \"\"\"\n",
    "    The main SCD object.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_graph,\n",
    "                 verbose=True,\n",
    "                 node_names=None,\n",
    "                 input_type=\"sparse_matrix\",\n",
    "                 device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Initiator class\n",
    "        \"\"\"\n",
    "        if input_type == \"networkx\":\n",
    "            node_names = list(input_graph.nodes())\n",
    "            input_graph = nx.to_scipy_sparse_matrix(input_graph)\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "        self.all_scores = None\n",
    "        self.node_names = node_names\n",
    "        self.cluster_quality = {}\n",
    "        self.default_parameters = {\n",
    "            \"clustering_scheme\": \"hierarchical\",\n",
    "            \"max_com_num\": \"100\",\n",
    "            \"verbose\": False,\n",
    "            \"sparisfy\": True,\n",
    "            \"parallel_step\": 6,\n",
    "            \"prob_threshold\": 0.0005,\n",
    "            \"community_range\": [1, 3, 5, 7, 11, 20, 40, 50, 100, 200, 300],\n",
    "            \"fine_range\": 3,\n",
    "            \"lag_threshold\": 10,\n",
    "            \"num_important\": 10000\n",
    "        }\n",
    "        self.input_graph = input_graph\n",
    "        self.quality_scores = {}\n",
    "\n",
    "    def page_rank_kernel(self, index_row):\n",
    "        \"\"\"\n",
    "        a kernel for parallel PPRS execution\n",
    "        param: index of a node.\n",
    "        output: PPR vector\n",
    "        \"\"\"\n",
    "\n",
    "        pr = self.PPRS([index_row],\n",
    "                       epsilon=1e-6,\n",
    "                       max_steps=100000,\n",
    "                       damping=0.90,\n",
    "                       spread_step=10,\n",
    "                       spread_percent=0.1,\n",
    "                       try_shrink=True)\n",
    "        norm = np.linalg.norm(pr, 2)\n",
    "        if norm > 0:\n",
    "            pr = pr / np.linalg.norm(pr, 2)\n",
    "            return (index_row, pr)\n",
    "        else:\n",
    "            return (index_row, np.zeros(self.normalized.shape[1]))\n",
    "\n",
    "    def sparse_pr(self, max_iter=10000, tol=1.0e-6, alpha=0.85):\n",
    "        \"\"\"\n",
    "        Sparse personalized pagerank.\n",
    "        \"\"\"\n",
    "\n",
    "        N = self.input_graph.shape[1]\n",
    "        nodelist = np.arange(self.input_graph.shape[1])\n",
    "        S = spy.array(self.input_graph.sum(axis=1)).flatten()\n",
    "        S[S != 0] = 1.0 / S[S != 0]\n",
    "        Q = sp.spdiags(S.T, 0, *self.input_graph.shape, format='csr')\n",
    "        M = Q * self.input_graph\n",
    "        x = spy.repeat(1.0 / N, N)\n",
    "        p = spy.repeat(1.0 / N, N)\n",
    "        dangling_weights = p\n",
    "        is_dangling = spy.where(S == 0)[0]\n",
    "        for _ in range(max_iter):\n",
    "            xlast = x\n",
    "            x = alpha * (x * M + sum(x[is_dangling]) * dangling_weights) + \\\n",
    "                (1 - alpha) * p\n",
    "            err = spy.absolute(x - xlast).sum()\n",
    "            if err < N * tol:\n",
    "                return dict(zip(nodelist, map(float, x)))\n",
    "\n",
    "    def stochastic_normalization(self, matrix=None, return_matrix=False):\n",
    "        \"\"\"\n",
    "        Perform stochastic normalization\n",
    "        \"\"\"\n",
    "\n",
    "        if matrix is None:\n",
    "            matrix = self.input_graph.tolil()\n",
    "        try:\n",
    "            matrix.setdiag(0)\n",
    "        except TypeError as te:\n",
    "            matrix.setdiag(np.zeros(matrix.shape[0]))\n",
    "        matrix = matrix.tocsr()\n",
    "        d = matrix.sum(axis=1).getA1()\n",
    "        nzs = np.where(d > 0)\n",
    "        k = 1 / d[nzs]\n",
    "        matrix = (sp.diags(k, 0).tocsc().dot(matrix)).transpose()\n",
    "        if return_matrix:\n",
    "            return matrix\n",
    "        else:\n",
    "            self.normalized = matrix\n",
    "\n",
    "    def modularity(self, communities, weight='weight'):\n",
    "        \"\"\"\n",
    "        Classic computation of modularity.\n",
    "        \"\"\"\n",
    "\n",
    "        G = nx.from_scipy_sparse_matrix(self.input_graph)\n",
    "        multigraph = G.is_multigraph()\n",
    "        directed = G.is_directed()\n",
    "        m = G.size(weight=weight)\n",
    "        if directed:\n",
    "            out_degree = dict(G.out_degree(weight=weight))\n",
    "            in_degree = dict(G.in_degree(weight=weight))\n",
    "            norm = 1 / m\n",
    "        else:\n",
    "            out_degree = dict(G.degree(weight=weight))\n",
    "            in_degree = out_degree\n",
    "            norm = 1 / (2 * m)\n",
    "\n",
    "        def val(u, v):\n",
    "            try:\n",
    "                if multigraph:\n",
    "                    w = sum(d.get(weight, 1) for k, d in G[u][v].items())\n",
    "                else:\n",
    "                    w = G[u][v].get(weight, 1)\n",
    "            except KeyError:\n",
    "                w = 0\n",
    "            # Double count self-loops if the graph is undirected.\n",
    "            if u == v and not directed:\n",
    "                w *= 2\n",
    "            return w - in_degree[u] * out_degree[v] * norm\n",
    "\n",
    "        Q = np.sum(\n",
    "            val(u, v) for c in communities for u, v in product(c, repeat=2))\n",
    "\n",
    "        return Q * norm\n",
    "\n",
    "    def PPRS(self,\n",
    "             start_nodes,\n",
    "             epsilon=1e-6,\n",
    "             max_steps=100000,\n",
    "             damping=0.5,\n",
    "             spread_step=10,\n",
    "             spread_percent=0.3,\n",
    "             try_shrink=True):\n",
    "        \"\"\"\n",
    "        Personalized PageRank with shrinking\n",
    "        \"\"\"\n",
    "\n",
    "        assert (len(start_nodes)) > 0\n",
    "        size = self.normalized.shape[0]\n",
    "        if start_nodes is None:\n",
    "            start_nodes = range(size)\n",
    "            nz = size\n",
    "        else:\n",
    "            nz = len(start_nodes)\n",
    "        start_vec = np.zeros((size, 1))\n",
    "        start_vec[start_nodes] = 1\n",
    "        start_rank = start_vec / len(start_nodes)\n",
    "        rank_vec = start_vec / len(start_nodes)\n",
    "        shrink = False\n",
    "        which = np.zeros(0)\n",
    "        if len(\n",
    "                self.global_top_nodes\n",
    "        ) < self.normalized.shape[1] and self.sample_whole_graph == False:\n",
    "            which = np.full(self.normalized.shape[1], False)\n",
    "            which[self.global_top_nodes] = True\n",
    "            start_rank = start_rank[which]\n",
    "            rank_vec = rank_vec[which]\n",
    "            self.normalized = self.normalized[:, which][which, :]\n",
    "            start_vec = start_vec[which]\n",
    "            size = len(self.global_top_nodes)\n",
    "        else:\n",
    "            which = np.zeros(0)\n",
    "        if try_shrink:\n",
    "            v = start_vec / len(start_nodes)\n",
    "            steps = 0\n",
    "            while nz < size * spread_percent and steps < spread_step:\n",
    "                steps += 1\n",
    "                v += self.normalized.dot(v)\n",
    "                nz_new = np.count_nonzero(v)\n",
    "                if nz_new == nz:\n",
    "                    shrink = True\n",
    "                    break\n",
    "                nz = nz_new\n",
    "            rr = np.arange(self.normalized.shape[0])\n",
    "            which = (v[rr] > 0).reshape(size)\n",
    "            if shrink:\n",
    "                start_rank = start_rank[which]\n",
    "                rank_vec = rank_vec[which]\n",
    "                self.normalized = self.normalized[:, which][which, :]\n",
    "        diff = np.Inf\n",
    "        steps = 0\n",
    "        while diff > epsilon and steps < max_steps:\n",
    "            steps += 1\n",
    "            new_rank = self.normalized.dot(rank_vec)\n",
    "            rank_sum = np.sum(new_rank)\n",
    "            if rank_sum < 0.999999999:\n",
    "                new_rank += start_rank * (1 - rank_sum)\n",
    "            new_rank = damping * new_rank + (1 - damping) * start_rank\n",
    "            new_diff = np.linalg.norm(rank_vec - new_rank, 1)\n",
    "            diff = new_diff\n",
    "            rank_vec = new_rank\n",
    "        if try_shrink and shrink:\n",
    "            ret = np.zeros(size)\n",
    "            rank_vec = rank_vec.T[0]\n",
    "            ret[which] = rank_vec\n",
    "            if start_nodes[0] < len(ret):\n",
    "                ret[start_nodes] = 0\n",
    "            return ret.flatten()\n",
    "        else:\n",
    "            if start_nodes[0] < len(rank_vec):\n",
    "                rank_vec[start_nodes] = 0\n",
    "            return rank_vec.flatten()\n",
    "\n",
    "    def emit(self, message):\n",
    "        \"\"\"\n",
    "        Simple logging wrapper\n",
    "        \"\"\"\n",
    "        logging.info(message)\n",
    "\n",
    "    def get_sparse_walk_matrix(self,\n",
    "                               num_important,\n",
    "                               prob_threshold=0,\n",
    "                               parallel_step=6):\n",
    "        \"\"\"\n",
    "        Get walk matrix\n",
    "        \"\"\"\n",
    "\n",
    "        if self.verbose:\n",
    "            self.emit(\"Walking..\")\n",
    "\n",
    "        if self.node_names is None:\n",
    "            self.node_names = list(range(self.input_graph.shape[1]))\n",
    "        self.global_pagerank = sorted(self.sparse_pr().items(),\n",
    "                                      key=operator.itemgetter(1),\n",
    "                                      reverse=True)\n",
    "        self.global_top_nodes = [\n",
    "            x[0] for x in self.global_pagerank[0:num_important]\n",
    "        ]\n",
    "        self.stochastic_normalization()\n",
    "        n = self.normalized.shape[1]\n",
    "        edgelist_triplets = []\n",
    "        jobs = [range(n)[i:i + parallel_step] for i in self.global_top_nodes]\n",
    "        with mp.Pool(processes=parallel_step) as p:\n",
    "            for batch in tqdm.tqdm(jobs):\n",
    "                results = p.map(self.page_rank_kernel, batch)\n",
    "                for nid, result_vector in results:\n",
    "                    cols = np.argwhere(\n",
    "                        result_vector > prob_threshold).flatten().astype(int)\n",
    "                    vals = result_vector[cols].flatten()\n",
    "                    ixx = np.repeat(nid, cols.shape[0]).flatten().astype(int)\n",
    "                    arx = np.vstack((ixx, cols, vals)).T\n",
    "                    edgelist_triplets.append(arx)\n",
    "        sparse_edgelist = np.concatenate(edgelist_triplets, axis=0)\n",
    "        print(\"Compressed to {}% of the initial size\".format(\n",
    "            (sparse_edgelist.shape[0] * 100) / n**2))\n",
    "        vectors = sp.coo_matrix(\n",
    "            (sparse_edgelist[:, 2], (sparse_edgelist[:, 0].astype(int),\n",
    "                                     sparse_edgelist[:,\n",
    "                                                     1].astype(int)))).tocsr()\n",
    "        return vectors\n",
    "\n",
    "    def approximate_normalized_graph_laplacian(self, A, rank, which=\"LA\"): \n",
    "        \n",
    "        n = A.shape[0]\n",
    "        L, d_rt = csgraph.laplacian(A, normed=True, return_diag=True)\n",
    "        \n",
    "        X = sp.identity(n) - L\n",
    "        self.emit(\"Eigen decomposition...\")\n",
    "        evals, evecs = eigsh(X, rank, which=which, sigma=0, tol=1E-2)\n",
    "        self.emit(\"Maximum eigenvalue {}, minimum eigenvalue {}\".format(\n",
    "            np.max(evals), np.min(evals)))\n",
    "        self.emit(\"Computing D^{-1/2}U..\")\n",
    "        D_rt_inv = sp.diags(d_rt**-1)\n",
    "        D_rt_invU = D_rt_inv.dot(evecs)\n",
    "        return evals, D_rt_invU\n",
    "    \n",
    "        \"\"\"\n",
    "        # NOTE: Swith to this version if encounter singular factor problem\n",
    "        # Compute the graph Laplacian\n",
    "        n = A.shape[0]\n",
    "        L, d_rt = csgraph.laplacian(A, normed=True, return_diag=True)\n",
    "\n",
    "        # Scale the Laplacian matrix to improve conditioning\n",
    "        scaling_factor = np.max(np.abs(d_rt))  # Find the largest diagonal element\n",
    "        scaled_L = L / scaling_factor  # Scale the Laplacian\n",
    "\n",
    "        X = sp.identity(n) - scaled_L\n",
    "        self.emit(\"Eigen decomposition...\")\n",
    "        evals, evecs = eigsh(X, rank, which=which, sigma=0, tol=1E-2)\n",
    "        self.emit(\"Maximum eigenvalue {}, minimum eigenvalue {}\".format(\n",
    "            np.max(evals), np.min(evals)))\n",
    "        self.emit(\"Computing D^{-1/2}U..\")\n",
    "        D_rt_inv = sp.diags(d_rt**-1)\n",
    "        D_rt_invU = D_rt_inv.dot(evecs)\n",
    "        return evals, D_rt_invU\n",
    "        \"\"\"\n",
    "\n",
    "    def approximate_deepwalk_matrix(self, evals, D_rt_invU, window, vol, b):\n",
    "        evals = self.deepwalk_filter(evals, window=window)\n",
    "        X = sp.diags(np.sqrt(evals)).dot(D_rt_invU.T).T\n",
    "        device = torch.device(\n",
    "            self.device)  #('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        m = torch.from_numpy(X).to(device)\n",
    "        vx = torch.tensor(vol / b).to(device)\n",
    "        mmt = torch.mm(m, m.t()).double()\n",
    "        vol = vx.expand_as(mmt).double()\n",
    "        mmt2 = mmt * vol\n",
    "        Y = torch.log(torch.clamp(mmt2, min=1)).cpu()\n",
    "        self.emit(\"Computed DeepWalk matrix with {} non-zero elements\".format(\n",
    "            np.count_nonzero(Y)))\n",
    "        return sp.csr_matrix(Y)\n",
    "\n",
    "    def svd_deepwalk_matrix(self, X, dim):\n",
    "        self.emit(\"Computing SVD..\")\n",
    "        u, s, v = svds(X, dim, return_singular_vectors=\"u\")\n",
    "        return sp.diags(np.sqrt(s)).dot(u.T).T\n",
    "\n",
    "    def deepwalk_filter(self, evals, window):\n",
    "        for i in range(len(evals)):\n",
    "            x = evals[i]\n",
    "            evals[i] = 1. if x >= 1 else x * (1 - x**window) / (1 - x) / window\n",
    "        evals = np.maximum(evals, 0)\n",
    "        self.emit(\n",
    "            \"After filtering, max eigenvalue={}, min eigenvalue={}\".format(\n",
    "                np.max(evals), np.min(evals)))\n",
    "        return evals\n",
    "\n",
    "    def netMF_large(self,\n",
    "                    A,\n",
    "                    rank=256,\n",
    "                    embedding_dimension=128,\n",
    "                    window=10,\n",
    "                    negative=1.0):\n",
    "\n",
    "        self.emit(\"Running TorchNetMF for a large window size...\")\n",
    "        self.emit(\"Window size is set to be {}\".format(window))\n",
    "\n",
    "        if rank >= A.shape[1]:\n",
    "            rank = A.shape[1] - 1\n",
    "\n",
    "        #if embedding_dimension >= A.shape[1]:\n",
    "        if embedding_dimension >= rank:\n",
    "            embedding_dimension = rank\n",
    "\n",
    "        # load adjacency matrix\n",
    "        vol = float(A.sum())\n",
    "\n",
    "        # perform eigen-decomposition of D^{-1/2} A D^{-1/2}\n",
    "        # keep top #rank eigenpairs\n",
    "\n",
    "        evals, D_rt_invU = self.approximate_normalized_graph_laplacian(\n",
    "            A, rank=rank, which=\"LA\")\n",
    "\n",
    "        # approximate deepwalk matrix\n",
    "        deepwalk_matrix = self.approximate_deepwalk_matrix(evals,\n",
    "                                                           D_rt_invU,\n",
    "                                                           window=window,\n",
    "                                                           vol=vol,\n",
    "                                                           b=negative)\n",
    "\n",
    "        # factorize deepwalk matrix with SVD\n",
    "        return self.svd_deepwalk_matrix(deepwalk_matrix,\n",
    "                                        dim=embedding_dimension)\n",
    "\n",
    "    def cross_entropy(self, data, clusters):\n",
    "        \"\"\"\n",
    "        The symmetric cross entropy is computed as follows:\n",
    "        SymCE = - sum_{i} (P(i) + Q(i)(log(P(i))+log(Q(i))\n",
    "\n",
    "        Idea:\n",
    "        for each potential cluster:\n",
    "        compute intra cluster cross entropy\n",
    "        Min intra, max inter.\n",
    "        \"\"\"\n",
    "\n",
    "        per_cluster = defaultdict(list)\n",
    "        unique_clusters = set(clusters)\n",
    "        clusters = np.array(clusters)\n",
    "        internal_entropies = []\n",
    "        for label in unique_clusters:\n",
    "            indices = np.where(clusters == label)\n",
    "            cmat = data[indices]\n",
    "            flat_vals = np.array(cmat.todense().flatten())\n",
    "            entropy_internal = -np.sum(\n",
    "                np.multiply(flat_vals, np.log(flat_vals)))\n",
    "            if np.isnan(entropy_internal):\n",
    "                entropy_internal = 1\n",
    "            internal_entropies.append(entropy_internal)\n",
    "        joint_entropy = 1 / np.mean(internal_entropies)\n",
    "        return joint_entropy\n",
    "\n",
    "    def compute_cluster_quality(self,\n",
    "                                clusters,\n",
    "                                weight=\"weight\",\n",
    "                                optional_data=None):\n",
    "\n",
    "        if self.clustering_measure == \"euclidean\":\n",
    "            distances = cdist(clusters, clusters, 'euclidean')\n",
    "            max_dist = np.max(distances)  #/np.mean(distances)\n",
    "            return max_dist\n",
    "\n",
    "        elif self.clustering_measure == \"modularity\":\n",
    "            return self.modularity(clusters, weight)\n",
    "\n",
    "        elif self.clustering_measure == \"silhouette\":\n",
    "            return silhouette_score(optional_data, clusters)\n",
    "\n",
    "        elif self.clustering_measure == \"davies_bouldin\":\n",
    "            return davies_bouldin_score(optional_data, clusters)\n",
    "\n",
    "        elif self.clustering_measure == \"calinski_harabasz\":\n",
    "            return calinski_harabasz_score(optional_data, clusters)\n",
    "\n",
    "        elif self.clustering_measure == \"entropy\":\n",
    "            return self.cross_entropy(optional_data, clusters)\n",
    "\n",
    "        else:\n",
    "            self.emit(\"Please, select a quality measure!\")\n",
    "\n",
    "    def kmeans_clustering(self,\n",
    "                          vectors,\n",
    "                          community_range,\n",
    "                          stopping,\n",
    "                          improvement_step=0.001,\n",
    "                          return_score=False):\n",
    "\n",
    "        if self.verbose:\n",
    "            self.emit(\"Doing kmeans search\")\n",
    "\n",
    "        if len(community_range) == 1:\n",
    "            fine_interval = 0\n",
    "\n",
    "        else:\n",
    "            fine_interval = int((community_range[2] - 1) / 2)\n",
    "            community_range = np.arange(community_range[0], community_range[1],\n",
    "                                        community_range[2])\n",
    "\n",
    "        nopt = 0\n",
    "        lag_num = 0\n",
    "        mx_opt = 0\n",
    "        opt_partitions = None\n",
    "        fine_range = None\n",
    "        all_scores = []\n",
    "\n",
    "        for nclust in community_range:\n",
    "            dx_hc = defaultdict(list)\n",
    "            clustering_algorithm = MiniBatchKMeans(n_clusters=nclust,\n",
    "                                                   init_size=3 * nclust,n_init=\"auto\")\n",
    "            clusters = clustering_algorithm.fit_predict(vectors).tolist()\n",
    "            for a, b in zip(clusters, self.node_names):\n",
    "                dx_hc[a].append(b)\n",
    "            partitions = dx_hc\n",
    "            lx = np.max([len(y) for x, y in partitions.items()])\n",
    "            if self.clustering_measure in [\"silhouette\", \"davies_bouldin\", \"calinski_harabasz\", \"entropy\"]:\n",
    "                mx = self.compute_cluster_quality(clusters,\n",
    "                                                  optional_data=vectors)\n",
    "            else:\n",
    "                mx = self.compute_cluster_quality(\n",
    "                    clustering_algorithm.cluster_centers_)\n",
    "            self.cluster_quality[str(nclust)] = mx\n",
    "            all_scores.append((nclust, mx))\n",
    "            if mx > mx_opt + improvement_step:\n",
    "                lag_num = 0\n",
    "                opt_partitions = partitions\n",
    "                nopt = nclust\n",
    "                if self.verbose:\n",
    "                    self.emit(\n",
    "                        \"Improved quality: {}, found {} communities. Maximum size {}\"\n",
    "                        .format(np.round(mx, 3), nopt, lx))\n",
    "                mx_opt = mx\n",
    "                self.opt_clust = partitions\n",
    "\n",
    "                #save for later use\n",
    "                self.clustering_algorithm = clustering_algorithm\n",
    "                self.nclust = nclust\n",
    "                self.vectors = vectors\n",
    "                self.clusters = clusters\n",
    "\n",
    "            else:\n",
    "                lag_num += 1\n",
    "                if self.verbose:\n",
    "                    self.emit(\n",
    "                        \"No improvement for {} iterations\".format(lag_num))\n",
    "            if lag_num == stopping:\n",
    "                if self.verbose:\n",
    "                    self.emit(\"Stopping criterion reached. Fine tunning.\")\n",
    "                    break\n",
    "                lag_num += 1\n",
    "\n",
    "        if self.verbose:\n",
    "            self.emit(\"Initial screening returned optimum of: {}\".format(nopt))\n",
    "\n",
    "        ## Do some fine tunning of the k\n",
    "        if fine_interval > 0:\n",
    "            fine_range = [\n",
    "                x for x in np.arange(nopt - fine_interval, nopt +\n",
    "                                     fine_interval, 1) if x > 0\n",
    "            ]\n",
    "            for cand in tqdm.tqdm(fine_range):\n",
    "                dx_hc = defaultdict(list)\n",
    "                clustering_algorithm = MiniBatchKMeans(n_clusters=nclust,\n",
    "                                                       init_size=3 * nclust,n_init=\"auto\")\n",
    "                clusters = clustering_algorithm.fit_predict(vectors).tolist()\n",
    "                for a, b in zip(clusters, self.node_names):\n",
    "                    dx_hc[a].append(b)\n",
    "                partitions = dx_hc\n",
    "                if self.clustering_measure in [\"silhouette\", \"davies_bouldin\", \"calinski_harabasz\", \"entropy\"]:\n",
    "                    mx = self.compute_cluster_quality(clusters,\n",
    "                                                      optional_data=vectors)\n",
    "                else:\n",
    "                    mx = self.compute_cluster_quality(\n",
    "                        clustering_algorithm.cluster_centers_)\n",
    "                if mx > mx_opt + improvement_step:\n",
    "                    lag_num = 0\n",
    "                    opt_partitions = partitions\n",
    "                    if self.verbose:\n",
    "                        self.emit(\n",
    "                            \"Improved quality: {}, found {} communities.\".\n",
    "                            format(np.round(mx, 2), len(partitions)))\n",
    "                    mx_opt = mx\n",
    "                    self.opt_clust = dx_hc\n",
    "                    nopt = nclust\n",
    "\n",
    "        if return_score:\n",
    "            return opt_partitions, mx_opt, all_scores\n",
    "\n",
    "        else:\n",
    "            return opt_partitions\n",
    "\n",
    "    def get_exp_features(self, cutoff=0.01, order=2):\n",
    "\n",
    "        core_graph = self.input_graph.tocsc()\n",
    "        feature_matrices = [core_graph]\n",
    "        for j in range(order):\n",
    "            if self.verbose:\n",
    "                self.emit(\"Order of computation: {}\".format(j))\n",
    "            new = sp.linalg.expm(feature_matrices[j])\n",
    "            feature_matrices.append(new)\n",
    "        final_matrix = feature_matrices[0]\n",
    "        for en, j in enumerate(feature_matrices):\n",
    "            if en > 0:\n",
    "                tar = self.stochastic_normalization(j.tocsr(),\n",
    "                                                    return_matrix=True)\n",
    "                final_matrix = final_matrix.tocsr().multiply(tar)\n",
    "        return final_matrix.tocsr()\n",
    "\n",
    "    def list_arguments(self):\n",
    "        argument_list = {\n",
    "            \"verbose\": False,\n",
    "            \"sparisfy\": True,\n",
    "            \"parallel_step\": 6,\n",
    "            \"prob_threshold (only for ppr_embedding)\": 0.0005,\n",
    "            \"community_range\": \"auto\",\n",
    "            \"num_important (only for PPR)\": 100,\n",
    "            \"clustering_measure\": \"silhouette\",\n",
    "            \"stopping\": 5,\n",
    "            \"improvement_step\": 0.05,\n",
    "            \"node_feature_type (netmf_embedding or ppr_embedding)\":\n",
    "            \"netmf_embedding\",\n",
    "            \"negative_range (negative sampling range)\": [1],\n",
    "            \"window_sizes (range of netmf window sizes)\": [3],\n",
    "            \"dims (latent dimension for netmf)\": [64],\n",
    "            \"output_format (grouped or nongrouped)\": \"nongrouped\",\n",
    "            \"use_normalized_scores (bool, normalize per netmf hyperparams)\":\n",
    "            True\n",
    "        }\n",
    "        for k, v in argument_list.items():\n",
    "            print(k, \"default setting:\", v)\n",
    "\n",
    "    def detect_communities(self,\n",
    "                           verbose=False,\n",
    "                           sparisfy=True,\n",
    "                           parallel_step=6,\n",
    "                           prob_threshold=0.0005,\n",
    "                           community_range=\"auto\",\n",
    "                           num_important=100,\n",
    "                           clustering_measure=\"silhouette\",\n",
    "                           stopping=5,\n",
    "                           improvement_step=0.05,\n",
    "                           node_feature_type=\"netmf_embedding\",\n",
    "                           negative_range=[1],\n",
    "                           window_sizes=[3],\n",
    "                           dims=[64],\n",
    "                           output_format=\"nongrouped\",\n",
    "                           use_normalized_scores=True,\n",
    "                           custom_embedding_vectors=None):\n",
    "\n",
    "        if community_range == \"auto\":\n",
    "            K = self.input_graph.shape[1]\n",
    "            kpow = int(0.42 * np.power(K, 2 / 3) - 5.7)\n",
    "            community_range = [kpow, K, kpow]\n",
    "            self.emit(f\"Estimated range to be explored: {community_range}\")\n",
    "\n",
    "        if self.verbose:\n",
    "            self.emit(\"Important nodes: {}\".format(num_important))\n",
    "        self.clustering_measure = clustering_measure\n",
    "\n",
    "        ## step 1: embedding\n",
    "        self.sample_whole_graph = False\n",
    "        if node_feature_type == \"EXP\":\n",
    "            vectors = self.get_exp_features()\n",
    "        elif node_feature_type == \"netmf_embedding\":\n",
    "\n",
    "            ## optimize the joint space..\n",
    "            lopt = 0\n",
    "            best_partition = None\n",
    "            self.opt_score = -1\n",
    "            self.opt_k = -1\n",
    "            self.opt_mean = -1\n",
    "            all_scores = None\n",
    "            self.opt_trace = []\n",
    "            for n in negative_range:\n",
    "                for w in window_sizes:\n",
    "                    for d in dims:\n",
    "                        if self.verbose:\n",
    "                            self.emit(\"testing setting {} {} {}\".format(\n",
    "                                n, w, d))\n",
    "                        vectors = self.netMF_large(self.input_graph,\n",
    "                                                   embedding_dimension=d,\n",
    "                                                   window=w,\n",
    "                                                   negative=n)\n",
    "                        tmp_partition, score, score_dump = self.kmeans_clustering(\n",
    "                            vectors,\n",
    "                            community_range,\n",
    "                            stopping,\n",
    "                            improvement_step,\n",
    "                            return_score=True)\n",
    "                        \n",
    "                        if use_normalized_scores and len(score_dump) > 1:\n",
    "                            normalized_score = (score - np.min(score_dump)) / (\n",
    "                                np.max(score_dump) - np.min(score_dump))\n",
    "                        else:\n",
    "                            normalized_score = score\n",
    "\n",
    "                        norm_score_dump = (score_dump - np.min(score_dump)) / (\n",
    "                            np.max(score_dump) - np.min(score_dump))\n",
    "\n",
    "                        score_mean = np.mean(\n",
    "                            norm_score_dump\n",
    "                        )  ## this was added after publication as it works better!\n",
    "                        self.opt_trace.append({\n",
    "                            \"score\": normalized_score,\n",
    "                            \"negative\": n,\n",
    "                            \"window\": w,\n",
    "                            \"dimension\": d,\n",
    "                            \"smean\": score_mean\n",
    "                        })\n",
    "                        if normalized_score > self.opt_score and tmp_partition and score_mean > self.opt_mean:\n",
    "                            self.opt_mean = score_mean\n",
    "                            self.all_scores = score_dump\n",
    "                            self.opt_k = len(tmp_partition)\n",
    "                            best_partition = tmp_partition\n",
    "                            self.final_embedding = vectors\n",
    "                            self.opt_score = normalized_score\n",
    "                            self.opt_config = {\n",
    "                                \"score\": normalized_score,\n",
    "                                \"negative\": n,\n",
    "                                \"window\": w,\n",
    "                                \"dimension\": d,\n",
    "                                \"Num communities\": len(tmp_partition),\n",
    "                                \"smean\": score_mean\n",
    "                            }\n",
    "                        else:\n",
    "                            self.emit(\"Invalid embedding\")\n",
    "\n",
    "        elif node_feature_type == \"ppr_embedding\":\n",
    "            vectors = self.get_sparse_walk_matrix(num_important,\n",
    "                                                  prob_threshold,\n",
    "                                                  parallel_step)\n",
    "            self.emit(\"Starting cluster detection..\")\n",
    "            best_partition, score, score_dump = self.kmeans_clustering(\n",
    "                vectors,\n",
    "                community_range,\n",
    "                stopping,\n",
    "                improvement_step,\n",
    "                return_score=True)\n",
    "            self.opt_score = score\n",
    "\n",
    "        elif node_feature_type == \"custom_embedding\":\n",
    "            self.emit(\"Starting cluster detection..\")\n",
    "            best_partition, score, score_dump = self.kmeans_clustering(\n",
    "                custom_embedding_vectors,\n",
    "                community_range,\n",
    "                stopping,\n",
    "                improvement_step,\n",
    "                return_score=True)\n",
    "            self.opt_score = score\n",
    "\n",
    "        if self.verbose:\n",
    "            self.emit(\"Obtained vectors of shape {}\".format(vectors.shape))\n",
    "            self.emit(self.opt_config)\n",
    "\n",
    "        assert vectors.shape[0] == self.input_graph.shape[0]\n",
    "\n",
    "        \n",
    "        measures = [\"silhouette\", \"davies_bouldin\", \"calinski_harabasz\"]\n",
    "        for m in measures:\n",
    "            self.clustering_measure = m\n",
    "            score = self.compute_cluster_quality(self.clusters, optional_data=self.vectors)\n",
    "            self.quality_scores[m] = score\n",
    "            self.emit(\"{} score: {}\".format(m, score))\n",
    "        \n",
    "        if output_format == \"grouped\":\n",
    "            return best_partition\n",
    "\n",
    "        else:\n",
    "            out_struct = {}\n",
    "            for k, els in best_partition.items():\n",
    "                for el in els:\n",
    "                    out_struct[el] = k\n",
    "            return out_struct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18820dff",
   "metadata": {},
   "source": [
    "## FUNCTIONS TO READ INPUT FILES ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa89c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleandf(df):\n",
    "    df = df.apply(lambda x: x.fillna(''))\n",
    "    df = df.apply(lambda x: x.astype(str).str.lower())\n",
    "    #df = df.apply(lambda x: x.astype(str).str.replace('\\W', ' ', regex=True))\n",
    "    df.replace('-', ' ', inplace=True) #replace all '-' values with NaN\n",
    "    df.dropna(how=\"all\", axis=1, inplace=True) #drop columns where all values are NaN\n",
    "    return df\n",
    "\n",
    "def extract(filename, encoding, sep):\n",
    "    df = pd.read_csv(filename, sep, engine='python', encoding = encoding)\n",
    "    return cleandf(df)\n",
    "\n",
    "def get_field_words(stemmer, lemmatizer, dataset):    \n",
    "    field_words = set()\n",
    "    ignores = ['label', 'class']\n",
    "    for c in dataset.columns:\n",
    "        #ignore 'label' or 'class' column-name\n",
    "        if c.strip().lower() in ignores:\n",
    "            #print(c)\n",
    "            continue        \n",
    "        words = [s for s in re.split('\\(|\\)|-|_|,|;|\\.|%|\\*', '_'.join(c.split('\\\\')))]\n",
    "        #print(f\"===============: {words}\")\n",
    "        words = [re.sub('[0-9]', '', i) for i in words]\n",
    "        words = [lemmatizer.lemmatize(w).upper() for w in words if len(w)>2 and w.isalnum()]\n",
    "        field_words.update(words)\n",
    "    return field_words\n",
    "\n",
    "def infer_types(aPath, csv_infer):\n",
    "    aprox_schema = csv_infer.run_inference(aPath)\n",
    "    typs = []\n",
    "    for k, v in aprox_schema.items():\n",
    "        typs.append(v.get('type'))\n",
    "    return typs\n",
    "\n",
    "# Function to convert inferred column types to pandas dtype\n",
    "def convert_to_dtype(inferred_type):\n",
    "    if inferred_type == \"INTEGER\":\n",
    "        return 'int64'\n",
    "    elif inferred_type == \"FLOAT\":\n",
    "        return 'float64'\n",
    "    elif inferred_type in [\"STRING\", \"DATE\"]:\n",
    "        return 'str'\n",
    "    elif inferred_type == \"DATETIME\":\n",
    "        return 'datetime64[ns]'\n",
    "    elif inferred_type == \"BOOLEAN\":\n",
    "        return 'bool'\n",
    "    else:\n",
    "        return 'str'  # Convert unknown types to string\n",
    "\n",
    "def get_IQR(column):\n",
    "    iqr=0\n",
    "    try:\n",
    "        q1 = column.quantile(0.25)\n",
    "        q3 = column.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        return iqr\n",
    "    except:\n",
    "        return iqr\n",
    "\n",
    "def get_sparsity(column):\n",
    "    missing_values = column.isnull().sum()\n",
    "\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values = column.count() + missing_values\n",
    "\n",
    "    # Calculate the sparsity\n",
    "    sparsity = (missing_values / total_values) * 100\n",
    "    return sparsity\n",
    "    \n",
    "def extract_metadata(in_column):\n",
    "    metadata_length = 10\n",
    "    metadata = [0] * metadata_length\n",
    "\n",
    "    column = in_column.dropna()\n",
    "\n",
    "    # meta-features\n",
    "    if column.dtype in ['int32', 'float32', 'int64', 'float64']:\n",
    "        metadata[0] = column.min()\n",
    "        metadata[1] = column.max()\n",
    "        metadata[2] = column.mean()\n",
    "        metadata[3] = column.median()\n",
    "        metadata[4] = column.std()\n",
    "        metadata[5] = column.skew()\n",
    "        metadata[6] = get_sparsity(column)\n",
    "    elif column.dtype in ['string', 'object', 'category']:\n",
    "        metadata[7] = len(column.unique())\n",
    "    elif column.dtype == 'datetime64':\n",
    "        metadata[8] = column.min().timestamp()\n",
    "        metadata[9] = column.max().timestamp()\n",
    "    elif column.dtype == 'timedelta64':\n",
    "        metadata[8] = column.min().total_seconds()\n",
    "        metadata[9] = column.max().total_seconds()\n",
    "    elif column.dtype == 'bool':\n",
    "        metadata[2] = column.mean()\n",
    "\n",
    "    # Convert the metadata list to a numpy array\n",
    "    metadata = np.array(metadata).reshape(-1, 1)\n",
    "    metadata = np.nan_to_num(metadata, nan=0.0)\n",
    "\n",
    "    # Scale the metadata using MinMaxScaler\n",
    "    #scaler = MinMaxScaler()\n",
    "    scaler = StandardScaler()\n",
    "    metadata = np.array(metadata).reshape(-1, 1)\n",
    "    metadata = scaler.fit_transform(metadata)\n",
    "    metadata = metadata.flatten().tolist()\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def get_bins_data(df, n_components=10):\n",
    "    # Initialize a dictionary to store the summarized metadata\n",
    "    all_metadata = {}\n",
    "\n",
    "    # Define the number of PCA components to keep\n",
    "    dataset_metadata = []  # Store metadata for the current dataset    \n",
    "    for col in df.columns:\n",
    "        column = df[col]\n",
    "        metadata = extract_metadata(column)  # The metadata contains a list of 10 bin values\n",
    "        n_components = len(df.columns) if len(df.columns) < n_components else n_components\n",
    "        dataset_metadata.extend(metadata)  # Extend the list with metadata values for this column\n",
    "\n",
    "    # Reshape the metadata to a 2D array (rows = columns, columns = metadata values)\n",
    "    dataset_metadata = np.array(dataset_metadata).reshape(len(df.columns), -1)\n",
    "\n",
    "    # Standardize the metadata values\n",
    "    scaler = StandardScaler()\n",
    "    dataset_metadata = scaler.fit_transform(dataset_metadata)\n",
    "\n",
    "    # Perform PCA to reduce dimensionality\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_result = pca.fit_transform(dataset_metadata)\n",
    "\n",
    "    # Store the PCA result in all_metadata\n",
    "    final_metadata = pca_result.tolist()\n",
    "    avg_pca_result = np.mean(final_metadata, axis=0)\n",
    "\n",
    "    bins = np.linspace(min(avg_pca_result), max(avg_pca_result), n_components + 1)\n",
    "    return np.digitize(avg_pca_result, bins) - 1\n",
    "\n",
    "def get_dataset_metadata(dataset):\n",
    "    dataset_metadata = []\n",
    "    for col in dataset.columns:\n",
    "        column = dataset[col]\n",
    "        metadata = extract_metadata(column)\n",
    "        dataset_metadata.extend(metadata)\n",
    "\n",
    "    return np.array(dataset_metadata)\n",
    "\n",
    "def read_it(datasets, directory, encoding, sep, max_size, stemmer, lemmatizer, csv_infer):\n",
    "    if os.path.isdir(directory):\n",
    "        for filename in os.listdir(directory):\n",
    "            pathfile = os.path.join(directory, filename)\n",
    "            isCSV = filename.endswith('.csv')\n",
    "            #print(isCSV)\n",
    "            isFile = os.path.isfile(pathfile)\n",
    "            #print(isFile)\n",
    "\n",
    "            if not isCSV or not isFile:\n",
    "                 datasets = read_it(datasets, pathfile, encoding, sep, max_size, stemmer, lemmatizer, csv_infer)\n",
    "            else:\n",
    "                #print(f\"Reading File: [{filename}]...\")\n",
    "                file_size = os.path.getsize(pathfile)\n",
    "                if (max_size != 0 and file_size > max_size):\n",
    "                    #print(f\"File Size is more than [{max_size}]. Skipped.\")\n",
    "                    continue\n",
    "                    \n",
    "                df = extract(pathfile, encoding, sep)\n",
    "                names = get_field_words(stemmer, lemmatizer, df)\n",
    "                if len(names)>0:\n",
    "                    types = infer_types(pathfile, csv_infer)\n",
    "                    metadata = get_dataset_metadata(df)\n",
    "                    datasets[os.path.splitext(filename)[0]] = [names, types, pathfile, metadata] #0=names, 1=types, 2=pathfile, 3=metadata\n",
    "                                        \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f003aed1",
   "metadata": {},
   "source": [
    "## FUNTIONS FOR LSH - MINHASH ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5970220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_func(size: int):\n",
    "    # function for creating the hash vector/function\n",
    "    hash_ex = list(range(1, size+1))\n",
    "    my_random.shuffle(hash_ex)\n",
    "    return hash_ex\n",
    "\n",
    "def build_minhash_func(vocabs_size: int, nbits: int):\n",
    "    # function for building multiple minhash vectors\n",
    "    hashes = []\n",
    "    for _ in range(nbits):\n",
    "        hashes.append(create_hash_func(vocabs_size))\n",
    "    return hashes\n",
    "\n",
    "def create_hash(vector: list, vocabs: list, minhash_func: list):\n",
    "    # use this function for creating our signatures (eg the matching)\n",
    "    signature = []\n",
    "    for func in minhash_func:\n",
    "        for i in range(1, len(vocabs)+1):\n",
    "            idx = func.index(i)\n",
    "            signature_val = vector[idx]\n",
    "            if signature_val == 1:\n",
    "                signature.append(idx)\n",
    "                break\n",
    "    return signature\n",
    "\n",
    "def split_vector(signature, b):\n",
    "    assert len(signature) % b == 0\n",
    "    r = int(len(signature) / b)\n",
    "    # code splitting signature in b parts\n",
    "    subvecs = []\n",
    "    for i in range(0, len(signature), r):\n",
    "        subvecs.append(signature[i : i+r])\n",
    "    return subvecs\n",
    "\n",
    "def probability(s, r, b):\n",
    "    # s: similarity\n",
    "    # r: rows (per band)\n",
    "    # b: number of bands\n",
    "    return 1 - (1 - s**r)**b\n",
    "\n",
    "def get_zips(nbr_minhash_funcs, nbr_bands, dict_records):\n",
    "    s_t = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "    start_time = time.perf_counter()\n",
    "    print(f\"Function:get_zips() started at {s_t}\")\n",
    "    \n",
    "    #create vocabs\n",
    "    vocabs = set()\n",
    "    for idx, rec in dict_records.items():\n",
    "        if len(rec)>0:\n",
    "            vocabs.update(rec)\n",
    "    vocabs = list(vocabs)\n",
    "    \n",
    "    # one-hot encoded the records\n",
    "    encodeds = {}\n",
    "    for idx, rec in dict_records.items():\n",
    "        encodeds[idx] = [1 if x in rec else 0 for x in vocabs]\n",
    "        \n",
    "    #create hash functions\n",
    "    minhash_func = build_minhash_func(len(vocabs), nbr_minhash_funcs)\n",
    "\n",
    "    #create signatures\n",
    "    signs = {}\n",
    "    for idx, enc in encodeds.items():\n",
    "        signs[idx] = create_hash(enc, vocabs, minhash_func)\n",
    "\n",
    "    #create bands\n",
    "    bands = {}\n",
    "    for idx, sign in signs.items():\n",
    "        #from signature with [nbr_minhash_funcs], split into [nbr_bands], each contain 5 rows\n",
    "        bands[idx] = split_vector(sign, nbr_bands)\n",
    "    \n",
    "    finish_time = time.perf_counter()\n",
    "    e_t = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "    print(f\"Function:get_zips() finished at {e_t} in {(finish_time-start_time)/60} minutes\")\n",
    "    return list(combinations(bands.items(), 2))\n",
    "\n",
    "def find_node(nodes: list, name: str):\n",
    "    for idx, node in nodes.items():\n",
    "        if name == node.name:\n",
    "            return node\n",
    "    return None\n",
    "\n",
    "def get_nodes_edges(result, nodes, edges):\n",
    "    for res_nodes, res_edge in result.get():\n",
    "        ns = list(res_nodes.items())\n",
    "        left = ns[0][0]\n",
    "        right = ns[1][0]\n",
    "        node_left = find_node(nodes, left)\n",
    "        if (not node_left):\n",
    "            nodes[left] = ns[0][1]\n",
    "        \n",
    "        node_right = find_node(nodes, right)\n",
    "        if (not node_right):\n",
    "            nodes[right] = ns[1][1]\n",
    "\n",
    "        if (res_edge.values()):\n",
    "            edges[list(res_edge.keys())[0]] = list(res_edge.values())[0]\n",
    "            \n",
    "#compute the Jaccard similarity to update the weight\n",
    "def jaccard_set(list1, list2):\n",
    "    \"\"\"Define Jaccard Similarity function for two sets\"\"\"\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    jaccard = len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "    simm = len(set1.symmetric_difference(set2))\n",
    "    len_diff = max(simm / len(set1.union(set2)), (min(len(list1), len(list2))/max(len(list1), len(list2))))\n",
    "    return jaccard*len_diff    \n",
    "    \n",
    "    #intersection = len(list(set(list1).intersection(list2)))\n",
    "    #union = (len(list1) + len(list2)) - intersection\n",
    "    #nList1 = len(list1)\n",
    "    #nList2 = len(list2)\n",
    "    #return float(intersection) / union * (min(nList1, nList2) / max(nList1, nList2))\n",
    "    \n",
    "def set_weight(edges, records, threshold=0.5):\n",
    "    for k, edge in edges.items():\n",
    "        curr_weight = edge.getWeight()\n",
    "        left = edge.node_left.name\n",
    "        left = list(records.get(left))\n",
    "        right = edge.node_right.name\n",
    "        right = list(records.get(right))\n",
    "        new_weight = jaccard_set(left, right)\n",
    "        weight = new_weight if new_weight > curr_weight else curr_weight\n",
    "        #weight = new_weight if curr_weight <= 0.0 else (curr_weight + new_weight)/2\n",
    "        if (weight) > threshold:\n",
    "            edge.setWeight(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b7b4f",
   "metadata": {},
   "source": [
    "## FUNCTIONS FOR LSH - RANDOM PROJECTIONS ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdea04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_trim_data(original_data, n_components):\n",
    "    if n_components < original_data.shape[0]:\n",
    "        # Trim the data to the desired number of components\n",
    "        padded_data = original_data[:n_components]\n",
    "    elif n_components > original_data.shape[0]:\n",
    "        # Calculate statistics from the original data\n",
    "        mean = np.mean(original_data)\n",
    "        std = np.std(original_data)\n",
    "        min_val = np.min(original_data)\n",
    "        max_val = np.max(original_data)\n",
    "\n",
    "        # Number of dimensions to pad\n",
    "        padding_dims = n_components - original_data.shape[0]\n",
    "\n",
    "        # Generate synthetic data with similar distribution\n",
    "        synthetic_data = []\n",
    "        for i in range(padding_dims):\n",
    "            # Generate synthetic values based on original data's statistics\n",
    "            #synthetic_values = my_random.normal(loc=mean, scale=std)\n",
    "            synthetic_values = my_random.gauss(mean, std)\n",
    "            synthetic_values = np.clip(synthetic_values, min_val, max_val)  # Ensure values are within original range\n",
    "            synthetic_data.append(synthetic_values)\n",
    "\n",
    "        # Combine original and synthetic data\n",
    "        padded_data = np.hstack((original_data, np.array(synthetic_data)))\n",
    "    else:\n",
    "        # No padding or trimming needed\n",
    "        padded_data = original_data\n",
    "\n",
    "    return padded_data\n",
    "\n",
    "def get_largest_shape(dfs_metadata):\n",
    "    largest_shape = 0\n",
    "    for k, df_metadata in dfs_metadata.items():\n",
    "        shape_0 = df_metadata.shape[0]\n",
    "        if shape_0 > largest_shape:\n",
    "            largest_shape = shape_0\n",
    "    print(f\"largest_shape == {largest_shape}\")\n",
    "    return largest_shape\n",
    "\n",
    "def get_best_nbits(n_samples, buckets=[2, 4, 8, 16, 24, 32, 64]):\n",
    "    best_nbits = 0\n",
    "    prev_samples_buckets = np.inf\n",
    "    for nbits in buckets:\n",
    "        buckets = 1 << nbits\n",
    "        samples_buckets = n_samples/buckets\n",
    "        print(f\"nbits == {nbits}\")\n",
    "        print(f\"{n_samples} / {buckets} = {samples_buckets}, samples_buckets: {samples_buckets}, prev_samples_buckets:{prev_samples_buckets}\")\n",
    "        if samples_buckets > 0.1 and samples_buckets < prev_samples_buckets:\n",
    "            prev_samples_buckets = samples_buckets\n",
    "            best_nbits = nbits\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    print(f\"best_nbits == {best_nbits}\")\n",
    "    return best_nbits\n",
    "\n",
    "\n",
    "def convert_keys_to_indices(data_dict):\n",
    "    indexed_dict = {}\n",
    "    for idx, (key, value) in enumerate(data_dict.items()):\n",
    "        indexed_dict[idx] = value\n",
    "    return indexed_dict\n",
    "\n",
    "def get_meta_edges(dfs_metadata_in):\n",
    "    keys = list(dfs_metadata_in.keys())\n",
    "    dfs_metadata = convert_keys_to_indices(dfs_metadata_in)\n",
    "    \n",
    "    largest_shape = get_largest_shape(dfs_metadata)\n",
    "    nbits = get_best_nbits(len(dfs_metadata))\n",
    "\n",
    "    d = largest_shape\n",
    "    #plane_norms = my_random.rand(nbits, d) - .5\n",
    "    plane_norms = np.array([[my_random.random() - 0.5 for _ in range(d)] for _ in range(nbits)])\n",
    "    \n",
    "    vectors = []\n",
    "    padded_metadata = []\n",
    "    for k, df_metadata in dfs_metadata.items():\n",
    "        padded_meta = pad_or_trim_data(df_metadata, largest_shape)\n",
    "        dot_data = np.dot(padded_meta, plane_norms.T)\n",
    "        dot_data = dot_data > 0\n",
    "        dot_data = dot_data.astype(int)\n",
    "        vectors.append(dot_data)\n",
    "        padded_metadata.append(padded_meta)\n",
    "    \n",
    "    buckets = {}\n",
    "    i = 0\n",
    "    for i in range(len(vectors)):\n",
    "        # convert from array to string\n",
    "        hash_str = ''.join(vectors[i].astype(str))\n",
    "        # create bucket if it doesn't exist\n",
    "        if hash_str not in buckets.keys():\n",
    "            buckets[hash_str] = []\n",
    "        # add vector position to bucket\n",
    "        buckets[hash_str].append(i)\n",
    "\n",
    "        \n",
    "    edges = {}\n",
    "    for k, v in buckets.items():\n",
    "        combs = list(combinations(v, 2))\n",
    "        for (l, r) in combs:\n",
    "            left = dfs_metadata[l].reshape(1, -1)\n",
    "            right = dfs_metadata[r].reshape(1, -1)\n",
    "            padded_left = padded_metadata[l].reshape(1, -1)\n",
    "            padded_right = padded_metadata[r].reshape(1, -1)\n",
    "            score = cosine_similarity(padded_left, padded_right)\n",
    "            edge = Edge(Node(keys[l]), Node(keys[r]), score[0, 0])\n",
    "            edges[create_edge_key(edge)] = edge\n",
    "                \n",
    "    #for key, edge in edges.items():\n",
    "    #    print(f\"key:{key} => {edge}\")\n",
    "        \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154158a6",
   "metadata": {},
   "source": [
    "## FUNCTIONS FOR PREPARATIONS ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f388de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_infer():\n",
    "    conditions={\"INTEGER\":\"FLOAT\"}\n",
    "    return csv_schema_inference.CsvSchemaInference(portion=0.9, max_length=100, batch_size = 200000, acc = 0.8, seed=2, header=True, sep=\",\", conditions = conditions)\n",
    "\n",
    "def read_files(directory, encoding='UTF-8', sep=',', max_size=0):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    csv_infer = get_csv_infer()\n",
    "    \n",
    "    datasets = {}\n",
    "    datasets = read_it(datasets, directory, encoding, sep, max_size, stemmer, lemmatizer, csv_infer)\n",
    "    return csv_infer, datasets\n",
    "\n",
    "def unpack_datasets(datasets):\n",
    "    # Initialize separate dictionaries for each variable\n",
    "    names = {}\n",
    "    types = {}\n",
    "    pathfiles = {}\n",
    "    dfs_metadata = {}\n",
    "\n",
    "    # Iterate through the original dictionary\n",
    "    for key, values in datasets.items():\n",
    "        names[key] = values[0]\n",
    "        types[key] = values[1]\n",
    "        pathfiles[key] = values[2]\n",
    "        dfs_metadata[key] = values[3]\n",
    "\n",
    "    return names, types, pathfiles, dfs_metadata\n",
    "\n",
    "def read_input(directory, fresh, encoding, sep, max_size, n_hash_fs, n_bands):\n",
    "    if fresh:\n",
    "        csv_infer, datasets = read_files(directory, encoding=encoding, sep=sep, max_size=max_size)\n",
    "        \n",
    "        with open(directory+'datasets.pkl', 'wb') as fp:\n",
    "            pickle.dump(datasets, fp)\n",
    "            print('datasets saved successfully to file')\n",
    "    \n",
    "    else:\n",
    "        csv_infer = get_csv_infer()\n",
    "        \n",
    "        # Read dictionary pkl file\n",
    "        with open(directory+'datasets.pkl', 'rb') as fp:\n",
    "            datasets = pickle.load(fp)\n",
    "            print('file datasets.pkl read successfully')\n",
    "\n",
    "    return csv_infer, datasets\n",
    "\n",
    "def merge_recs(rec_dict1, rec_dict2):\n",
    "    merged_recs = {}\n",
    "\n",
    "    # Add nodes from the first dictionary\n",
    "    for key, rec in rec_dict1.items():\n",
    "        merged_recs[key] = rec\n",
    "\n",
    "    # Add nodes from the second dictionary\n",
    "    for key, rec in rec_dict2.items():\n",
    "        if key not in merged_recs:\n",
    "            merged_recs[key] = rec\n",
    "\n",
    "    return merged_recs\n",
    "\n",
    "def merge_nodes(node_dict1, node_dict2):\n",
    "    merged_nodes = {}\n",
    "\n",
    "    # Add nodes from the first dictionary\n",
    "    for key, node in node_dict1.items():\n",
    "        merged_nodes[key] = node\n",
    "\n",
    "    # Add nodes from the second dictionary\n",
    "    for key, node in node_dict2.items():\n",
    "        if key not in merged_nodes:\n",
    "            merged_nodes[key] = node\n",
    "\n",
    "    return merged_nodes\n",
    "\n",
    "def update_edges(edge_dict1, edge_dict2, replace_only=False):\n",
    "    p_param = 0.5\n",
    "    merged_edges = {}\n",
    "    \n",
    "    # Add edges from the first dictionary\n",
    "    for key, edge in edge_dict1.items():\n",
    "        merged_edges[key] = edge\n",
    "    \n",
    "    # Add edges from the second dictionary\n",
    "    for key, edge in edge_dict2.items():\n",
    "        if key not in merged_edges and not replace_only:\n",
    "            merged_edges[key] = edge\n",
    "        elif key in merged_edges:\n",
    "            edge1 = merged_edges.get(key)\n",
    "            weight1 = edge1.getWeight()\n",
    "            edge2 = edge\n",
    "            weight2 = edge2.getWeight()\n",
    "            average_weight = weight1 if weight1 > weight2 else weight2\n",
    "            if replace_only:\n",
    "                weight2 = p_param*weight2\n",
    "                average_weight = weight1 if weight1 > weight2 else weight2\n",
    "            edge1.setWeight(average_weight)\n",
    "            \n",
    "    return merged_edges\n",
    "\n",
    "def do_LSH(datasets, n_hash_fs, n_bands):\n",
    "    nm_recs, ty_recs, pathfiles, dfs_metadata = unpack_datasets(datasets)\n",
    "    nm_zips = get_zips(n_hash_fs, n_bands, nm_recs)\n",
    "    ty_zips = get_zips(n_hash_fs, n_bands, ty_recs)\n",
    "    \n",
    "    s_t = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "    start_time = time.perf_counter()\n",
    "    print(f\"Function:LSH-based Similarity started at {s_t}\")\n",
    "    print(f\"Processing total datasets: {len(datasets)}\")\n",
    "\n",
    "    nm_nodes = {}\n",
    "    nm_edges = {}\n",
    "    ty_nodes = {}\n",
    "    ty_edges = {}\n",
    "\n",
    "    with mp.Pool() as pool:\n",
    "        result = pool.map_async(create_netw, nm_zips)\n",
    "        get_nodes_edges(result, nm_nodes, nm_edges)\n",
    "\n",
    "    with mp.Pool() as pool:\n",
    "        result = pool.map_async(create_netw, ty_zips)\n",
    "        get_nodes_edges(result, ty_nodes, ty_edges)\n",
    "\n",
    "    finish_time = time.perf_counter()\n",
    "    e_t = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "    print(f\"Function:LSH-based Similarity finished at {e_t} in {(finish_time-start_time)/60} minutes\")\n",
    "    \n",
    "    set_weight(nm_edges, nm_recs)\n",
    "    set_weight(ty_edges, ty_recs)\n",
    "    \n",
    "    meta_edges = get_meta_edges(dfs_metadata)\n",
    "    \n",
    "    nodes = merge_nodes(nm_nodes, ty_nodes)\n",
    "    edges = update_edges(nm_edges, ty_edges)\n",
    "    edges = update_edges(ty_edges, meta_edges, replace_only=True)\n",
    "    \n",
    "    print(f\"Number of nodes: {len(nodes)}\")\n",
    "    print(f\"Number of edges: {len(edges)}\")\n",
    "\n",
    "    return nm_nodes, nm_edges, nodes, edges\n",
    "\n",
    "def prepare(directory, fresh, encoding='ISO-8859-1', sep=',', max_size=500000, n_hash_fs=100, n_bands = 20):\n",
    "    csv_infer, datasets = read_input(directory, fresh, encoding, sep, max_size, n_hash_fs, n_bands)\n",
    "    ini_nodes, init_edges, merge_nodes, merge_edges = do_LSH(datasets, n_hash_fs, n_bands)\n",
    "\n",
    "    #for k, edge in edges.items():\n",
    "    #    print(edge)\n",
    "    #    break\n",
    "    \n",
    "    return csv_infer, datasets, ini_nodes, init_edges, merge_nodes, merge_edges\n",
    "\n",
    "def prepare_all(root_dirs, fresh, encoding='ISO-8859-1', sep=',', max_size=500000, n_hash_fs=100, n_bands = 20):\n",
    "    all_datasets = {}\n",
    "    csv_infer = None\n",
    "    \n",
    "    for k, values in root_dirs.items():\n",
    "        directory = values[0]\n",
    "        max_size = values[1]\n",
    "        csv_infer, datasets = read_input(directory, fresh, encoding, sep, max_size, n_hash_fs, n_bands)\n",
    "        print(f\"Adding {k} with datasets: {len(datasets)}\")\n",
    "        all_datasets.update(datasets)\n",
    "        \n",
    "    ini_nodes, init_edges, merge_nodes, merge_edges = do_LSH(all_datasets, n_hash_fs, n_bands)\n",
    "    \n",
    "    return csv_infer, all_datasets, ini_nodes, init_edges, merge_nodes, merge_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c0a18b",
   "metadata": {},
   "source": [
    "## GRAPH NETWORK ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd78ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(nodes, edges, merge, threshold=0.5):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for k, node in nodes.items():\n",
    "        G.add_node(node.name)\n",
    "\n",
    "    for k, edge in edges.items():\n",
    "        w = edge.edge_weight\n",
    "        if w > threshold:\n",
    "            a = edge.node_left\n",
    "            b = edge.node_right\n",
    "            G.add_edge(a.name, b.name, weight=w)\n",
    "            \n",
    "    if merge:\n",
    "        # Step 1: Remove node with degree < 2\n",
    "        nodes_to_remove = [node for node in G.nodes() if G.degree(node) < 2]\n",
    "        G.remove_nodes_from(nodes_to_remove)\n",
    "        \n",
    "        # Step 2: Find connected components in the modified graph\n",
    "        connected_cs = list(nx.connected_components(G))\n",
    "\n",
    "        # Step 3: Identify the connected component with the least number of members\n",
    "        min_size_component = min(connected_cs, key=len)\n",
    "\n",
    "        # Step 4: Add the removed nodes to the identified connected component\n",
    "        last_element = None\n",
    "        for node in nodes_to_remove:\n",
    "            G.add_node(node)  # Add the node\n",
    "            if min_size_component:\n",
    "                last_element = min_size_component.pop()\n",
    "                G.add_edge(node, last_element, weight=0.5)  # Add an edge to the chosen component\n",
    "            elif last_element:\n",
    "                G.add_edge(node, last_element, weight=0.5)  # Use the last element for the remaining nodes\n",
    "\n",
    "        # Step 1: Remove node with degree < 2\n",
    "        nodes_to_remove = [node for node in G.nodes() if G.degree(node) < 2]\n",
    "        G.remove_nodes_from(nodes_to_remove)\n",
    "        \n",
    "        # Step2: Find connected components with members less than 5\n",
    "        #small_components = [component for component in nx.connected_components(G) if len(component) < 5]\n",
    "\n",
    "        # Step 3: Remove nodes from small connected components\n",
    "        #nodes_to_remove = [node for component in small_components for node in component]\n",
    "        #G.remove_nodes_from(nodes_to_remove)\n",
    "        \n",
    "    return G\n",
    "\n",
    "def get_communities(G, nodes):\n",
    "    sparse_mat = nx.to_scipy_sparse_matrix(G)\n",
    "    node_names = list(range(sparse_mat.shape[1]))\n",
    "    SCD_detector = SCD_obj(sparse_mat,node_names=node_names)\n",
    "\n",
    "    ## all hyperparameters\n",
    "    SCD_detector.list_arguments()\n",
    "\n",
    "    ## set hyperparameters\n",
    "    param2 = {\"verbose\":True,\"parallel_step\":8,\"stopping\":25,\"use_normalized_scores\":False,\"community_range\":[2,len(G.nodes()),2]}\n",
    "    communities = SCD_detector.detect_communities(**param2)\n",
    "    \n",
    "    #communities = nx.community.greedy_modularity_communities(G, weight=\"weight\")\n",
    "    # Create a dictionary where each node is assigned to its own community\n",
    "    #node_to_community = {node: i for i, component in enumerate(nx.connected_components(G)) for node in component}\n",
    "    # Convert the dictionary to a list of communities\n",
    "    #communities = [set() for _ in range(max(node_to_community.values()) + 1)]\n",
    "    #for node, community in node_to_community.items():\n",
    "    #    communities[community].add(node)\n",
    "    \n",
    "    # Create a new dictionary to store the grouped keys\n",
    "    comms = {}\n",
    "\n",
    "    # Iterate through the original dictionary\n",
    "    for key, value in communities.items():\n",
    "        if value in comms:\n",
    "            comms[value].append(key)\n",
    "        else:\n",
    "            comms[value] = [key]\n",
    "    #print(f\"Communities before merging process: {comms}\")\n",
    "            \n",
    "    # Create a new dictionary to store merged results\n",
    "    merged_comms = {}    \n",
    "    key_counter = 0  # Counter for assigning new keys\n",
    "    for key, values in comms.items():\n",
    "        if len(values) < 2 and merged_comms:\n",
    "            # Find the key with the larger list to merge into\n",
    "            max_key = max(merged_comms.keys(), key=lambda k: len(merged_comms[k]))\n",
    "            # Merge the values into the key with the larger list\n",
    "            merged_comms[max_key].extend(values)\n",
    "        else:\n",
    "            # If list has 5 or more elements, add to merged_data as is\n",
    "            merged_comms[key_counter] = values\n",
    "            key_counter += 1\n",
    "    \n",
    "    #print(f\"Communities after merging process: {merged_comms}\")\n",
    "    \n",
    "    node_indices = {index: node_name for index, node_name in enumerate(G.nodes())}\n",
    "    final_communities = {}\n",
    "    for key, indices in merged_comms.items():\n",
    "        final_communities[key] = [{node_indices[index]: nodes[node_indices[index]]} for index in indices]\n",
    "    \n",
    "    #print(f\"Final Communities: {final_communities}\")\n",
    "    \n",
    "    return final_communities\n",
    "\n",
    "def convert(comm):\n",
    "    # Create a dictionary to store items grouped by their values\n",
    "    grouped_dict = {}\n",
    "    for key, value in comm.items():\n",
    "        if value in grouped_dict:\n",
    "            grouped_dict[value][key] = value\n",
    "        else:\n",
    "            grouped_dict[value] = {key: value}\n",
    "\n",
    "    return list(grouped_dict.values())\n",
    "\n",
    "def compute_metrics(other_methods, directory, fresh, encoding, sep, max_size, n_hash_fs, n_bands, root_dirs=False):\n",
    "    if root_dirs:\n",
    "        _, _, _, _, nodes, edges = prepare_all(directory, fresh, encoding, sep, max_size, n_hash_fs, n_bands)  \n",
    "    else:\n",
    "        _, _, _, _, nodes, edges = prepare(directory, fresh, encoding, sep, max_size, n_hash_fs, n_bands)  \n",
    "    G = get_graph(nodes, edges, True)\n",
    "    #pickle.dump(G, open(f'{directory}\\\\graph.pickle', 'wb'))\n",
    "    \n",
    "    sparse_mat = nx.to_scipy_sparse_matrix(G)\n",
    "    node_names = list(range(sparse_mat.shape[1]))\n",
    "    SCD_detector = SCD_obj(sparse_mat,node_names=node_names)\n",
    "    SCD_detector.list_arguments()\n",
    "    param2 = {\"verbose\":True,\"parallel_step\":8,\"stopping\":25,\"use_normalized_scores\":False,\"community_range\":[2,len(G.nodes()),2]}\n",
    "    communities = SCD_detector.detect_communities(**param2)\n",
    "    #print(f\"communities:{communities}\")\n",
    "    modularities = {}\n",
    "    if other_methods:\n",
    "        modularities['OUR'] = SCD_detector.modularity(convert(communities))\n",
    "        for m in other_methods:\n",
    "            if m == \"GMC\":\n",
    "                gmc_comm = nx.community.greedy_modularity_communities(G)            \n",
    "                #print(f\"gmc_comm:\\n{gmc_comm}\")\n",
    "                gmc = nx.community.modularity(G, gmc_comm)\n",
    "                modularities[m] = gmc\n",
    "            elif m == \"LPA\":\n",
    "                lpa_comm = nx.algorithms.community.label_propagation_communities(G)\n",
    "                #print(f\"lpa_comm:\\n{lpa_comm}\")\n",
    "                lpa = nx.community.modularity(G, lpa_comm)\n",
    "                modularities[m] = lpa\n",
    "            elif m == \"LVA\":\n",
    "                lva_comm = nx.algorithms.community.louvain_communities(G)\n",
    "                #print(f\"lva_comm:\\n{lva_comm}\")\n",
    "                lva = nx.community.modularity(G, lva_comm)\n",
    "                modularities[m] = lva\n",
    "\n",
    "    return SCD_detector.quality_scores, modularities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7765d2",
   "metadata": {},
   "source": [
    "## DRAW NETWORK ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9301ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(nodes, edges, merge=False, threshold=0.5, spaces=0.15, chart_size=[5,5], legend_pos=[0.95, 0.3, 0.03, 0.3], \n",
    "         legend_label='Connected Components', remarks_pos=[1.3, 0.55, 0.03, 0.3]):\n",
    "    G = get_graph(nodes, edges, merge, threshold)\n",
    "\n",
    "    pos = nx.spring_layout(G, k=spaces)\n",
    "    wcc = list(nx.connected_components(G))\n",
    "\n",
    "    # Set the number of colors\n",
    "    num_colors = len(wcc)\n",
    "    # Generate a set of visually distinct colors using seaborn's color palette\n",
    "    distinct_colors = sns.color_palette(\"husl\", n_colors=num_colors)\n",
    "    # Convert the colors to hexadecimal format for Networkx\n",
    "    distinct_colors_hex = ['#%02x%02x%02x' % (int(r*255), int(g*255), int(b*255)) for r, g, b in distinct_colors]\n",
    "    \n",
    "    \n",
    "    thick = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] >= 0.9]\n",
    "    medium = [(u, v) for (u, v, d) in G.edges(data=True) if 0.75 < d[\"weight\"] < 0.9]\n",
    "    thin = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] <= 0.75]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=chart_size)\n",
    "    ax.set_facecolor('white')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('black')\n",
    "\n",
    "    for index, sg in enumerate(wcc):\n",
    "        nx.draw_networkx_nodes(sg, pos=pos, node_size=75, edgecolors=\"black\", node_color=distinct_colors_hex[index])\n",
    "        nx.draw_networkx_edges(G, pos=pos, ax=ax, edgelist=thick, width=2, edge_color=\"lightblue\")        \n",
    "        nx.draw_networkx_edges(G, pos=pos, ax=ax, edgelist=medium, width=1, edge_color=\"lightgreen\")\n",
    "        nx.draw_networkx_edges(G, pos=pos, ax=ax, edgelist=thin, width=1, alpha=0.5, edge_color=\"lightgrey\", style=\"dashed\")\n",
    "        \n",
    "        \n",
    "    # add legends\n",
    "    # https://matplotlib.org/stable/tutorials/colors/colorbar_only.html\n",
    "    ax2 = fig.add_axes(legend_pos) # distance left, distance up, width size, height size\n",
    "    cmap = (colors.ListedColormap(distinct_colors_hex))\n",
    "    bounds = list(range(0, num_colors+1))\n",
    "    norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "    cbar = fig.colorbar(\n",
    "        plt.cm.ScalarMappable(cmap=cmap, norm=norm),\n",
    "        cax=ax2,\n",
    "        extend='neither',\n",
    "        ticks=bounds,\n",
    "        spacing='proportional',\n",
    "        orientation='vertical',\n",
    "\n",
    "    )\n",
    "    #cbar.ax.tick_params(labelsize=12)\n",
    "    # Hide tick labels while keeping the boundary lines\n",
    "    cbar.set_ticklabels([])\n",
    "    ax2.set_ylabel(legend_label, size=12)\n",
    "\n",
    "    ax3 = fig.add_axes(remarks_pos) # distance left, distance up, width size, height size\n",
    "    ax3.axis(\"off\")\n",
    "    \n",
    "    custom_lines = [Line2D([0], [0], color=\"lightblue\", lw=2),\n",
    "                    Line2D([0], [0], color=\"lightgreen\", lw=1),\n",
    "                    Line2D([0], [0], color=\"lightgrey\", lw=1, linestyle='dashed',)]\n",
    "\n",
    "    ax3.legend(custom_lines, ['feature_similarity >= 0.9', '0.75 < feature_similarity < 0.9', 'feature_similarity <= 0.75'],\n",
    "               loc='best', prop={'size': 10}, fancybox=True, framealpha=0)\n",
    "\n",
    "    #plt.savefig('Figure_node_lines.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return G\n",
    "\n",
    "def draw_net(ini_nodes, init_edges, merge_nodes, merge_edges, threshold=0.5, spaces=0.2, chart_size=[8,6], \n",
    "     legend_pos=[0.95, 0.15, 0.03, 0.5], legend_label1='Network Nodes & Edges', legend_label2='Connected Components', remarks_pos=[1.25, 0.55, 0.03, 0.3]):\n",
    "    results = pd.DataFrame({\n",
    "        's': [],\n",
    "        'P': [],\n",
    "        'r,b': []\n",
    "    })\n",
    "\n",
    "    for s in np.arange(0.01, 1, 0.01):\n",
    "        total = 100\n",
    "        factors = []\n",
    "        for i in range(1, total+1):\n",
    "            if total%i==0:\n",
    "                factors.append(i)\n",
    "        for b in factors:\n",
    "            r = int(total/b)\n",
    "            P = probability(s, r, b)\n",
    "            results = results.append({\n",
    "                's': s,\n",
    "                'P': P,\n",
    "                'r,b': f\"{r},{b}\"\n",
    "            }, ignore_index=True)\n",
    "\n",
    "    sns.lineplot(data=results, x='s', y='P', hue='r,b')\n",
    "        \n",
    "    draw(ini_nodes, init_edges, threshold=threshold, spaces=spaces, chart_size=chart_size, legend_pos=legend_pos, legend_label=legend_label1, remarks_pos=remarks_pos)\n",
    "    G = draw(merge_nodes, merge_edges, merge=True, threshold=threshold, spaces=spaces, chart_size=chart_size, legend_pos=legend_pos, legend_label=legend_label1, remarks_pos=remarks_pos)\n",
    "    communities = get_communities(G, merge_nodes)\n",
    "    \n",
    "    # create a list of colors from a matplotlib colormap\n",
    "    cmap = matplotlib.cm.get_cmap('tab20')\n",
    "\n",
    "    color_intervals = np.linspace(0,1,len(list(communities)))\n",
    "    colors_tab = []\n",
    "    for value in color_intervals:\n",
    "        colors_tab.append(cmap(value))\n",
    "\n",
    "    # add color to nodes by community group\n",
    "    community_color_map = []\n",
    "    for node in G.nodes:\n",
    "        for idx, community_list in communities.items():\n",
    "            community_keys = [list(comm_dict.keys())[0] for comm_dict in community_list]\n",
    "            #print(f'idx:{idx}, comm:{community_keys}')\n",
    "            if node in community_keys:\n",
    "                community_color_map.append(colors_tab[idx])\n",
    "\n",
    "    pos = nx.spring_layout(G, k=0.5) # added a seed for layout reproducibility\n",
    "    fig, (ax_data, ax_legend) = plt.subplots(1, 2, figsize=(8, 5), gridspec_kw={'width_ratios': [4, 1]})\n",
    "\n",
    "    # Create a legend for communities\n",
    "    legend_labels = [f\"Community {idx}\" for idx in range(len(communities))]\n",
    "    legend_handles = [matplotlib.patches.Patch(color=colors_tab[idx], label=label) for idx, label in enumerate(legend_labels)]\n",
    "\n",
    "    # Position the legend outside the chart area\n",
    "    ax_legend.legend(handles=legend_handles, bbox_to_anchor=(1, 1), loc='upper right')\n",
    "    ax_legend.axis(\"off\")\n",
    "\n",
    "    # Add a border around the entire figure\n",
    "    border_color = 'grey'  # Color of the border\n",
    "    border_width = 0.25  # Width of the border\n",
    "    fig.patch.set_edgecolor(border_color)\n",
    "    fig.patch.set_linewidth(border_width)\n",
    "\n",
    "    # Draw the networkx graph inside the ax_data subplot\n",
    "    nx.draw(G, pos, ax=ax_data, node_color=community_color_map, edge_color=\"lightgrey\", node_size=50, edgecolors='black', linewidths=0.25)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    #print(f\"Modularity: {round(nx_comm.modularity(G, communities),3)}\")\n",
    "    \n",
    "    return G, communities\n",
    "        \n",
    "def draw_all(csv_infer, datasets, ini_nodes, init_edges, merge_nodes, merge_edges, encoding='ISO-8859-1', sep=',', include_metadata=True, max_metadata_length=100, threshold=0.5, spaces=0.2, chart_size=[8,6], \n",
    "     legend_pos=[0.95, 0.15, 0.03, 0.5], legend_label1='Network Nodes & Edges', legend_label2='Connected Components', remarks_pos=[1.25, 0.55, 0.03, 0.3]):\n",
    "\n",
    "    #drwa network\n",
    "    G, communities = draw_net(ini_nodes, init_edges, merge_nodes, merge_edges, threshold, spaces, chart_size, legend_pos, legend_label1, legend_label2, remarks_pos)\n",
    "    \n",
    "    #create classifier model\n",
    "    untrained_model, community_labels, feature_vocab, X, y = build_classifier(csv_infer, datasets, G, communities, encoding, sep, include_metadata, max_metadata_length)\n",
    "    \n",
    "    #train k-fold\n",
    "    trained_kfold_model = train_kfold(community_labels, X, y)\n",
    "    \n",
    "    #train shuffle\n",
    "    trained_shuffle_model = train_shuffle(community_labels, X, y)\n",
    "    \n",
    "    return untrained_model, trained_kfold_model, trained_shuffle_model, communities, community_labels, feature_vocab, X, y    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab5d8f",
   "metadata": {},
   "source": [
    "## CLASSIFIER ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac18cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model\n",
    "class CommunityClassifier(nn.Module):\n",
    "    def __init__(self, in_feats, num_classes):\n",
    "        super(CommunityClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(in_feats, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def build_classifier(csv_infer, datasets, G, communities, encoding, sep, include_metadata, max_metadata_length):\n",
    "    # Create numerical community labels\n",
    "    community_labels = list(range(len(communities)))\n",
    "    # Print the numerical community labels to verify\n",
    "    #print(community_labels)\n",
    "\n",
    "    n_community_labels = torch.tensor(community_labels, dtype=torch.long)\n",
    "    #print(n_community_labels)\n",
    "\n",
    "    # Define a LabelEncoder for mapping categorical features to numerical values\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    # Unpack datasets\n",
    "    names, types, pathfiles, dfs_metadata = unpack_datasets(datasets)\n",
    "\n",
    "    community_features = []\n",
    "    for idx, community_list in communities.items():\n",
    "        community_keys = [list(comm_dict.keys())[0] for comm_dict in community_list]\n",
    "        c_feats = []\n",
    "        for c in community_keys:\n",
    "            nm = list(names[c])\n",
    "            typ = list(set(types[c]))\n",
    "            feature = nm + typ\n",
    "            c_feats.append({c:feature})\n",
    "        community_features.append({idx: c_feats})\n",
    "\n",
    "    #print(community_features)\n",
    "\n",
    "    # Extract features and labels\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_id_feature_mappings = {}    \n",
    "    \n",
    "    flat_features = []\n",
    "    for community_id, features_list in enumerate(community_features):\n",
    "        for feature_dict in features_list.values():\n",
    "            flat_features.extend((community_id, node_name, node_features) for node_info in feature_dict for node_name, node_features in node_info.items())\n",
    "\n",
    "    all_features = [node_features for _, _, node_features in flat_features]\n",
    "    all_labels = [community_id for community_id, _, _ in flat_features]\n",
    "    all_id_feature_mappings = {idx: {node_name: node_features} for idx, (_, node_name, node_features) in enumerate(flat_features)}\n",
    "\n",
    "    # Convert community labels to numerical values using LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    numerical_community_labels = label_encoder.fit_transform(all_labels)\n",
    "\n",
    "    # Convert features to a format suitable for training\n",
    "    feature_mapping = defaultdict(list)\n",
    "    for idx, features in enumerate(all_features):\n",
    "        feature_mapping[numerical_community_labels[idx]].extend(features)\n",
    "\n",
    "    # Create a vocabulary of unique features\n",
    "    unique_features = list(set(feature for features in feature_mapping.values() for feature in features))\n",
    "    feature_vocab = {feature: idx for idx, feature in enumerate(unique_features)}\n",
    "\n",
    "    # Convert feature lists to binary vectors using the vocabulary\n",
    "    binary_features = []\n",
    "    for features in all_features:\n",
    "        binary_features.append([1 if feature in features else 0 for feature in feature_vocab])\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X = torch.tensor(binary_features, dtype=torch.float)\n",
    "    #print(len(all_features))\n",
    "    #print(X.shape)\n",
    "    \n",
    "    if include_metadata:\n",
    "        metadata_tensors = []\n",
    "        for k, fp in pathfiles.items():\n",
    "            if (k not in G.nodes):\n",
    "                #it was pruned, skipped\n",
    "                continue\n",
    "\n",
    "            inferred_column_types = infer_types(fp, csv_infer)\n",
    "\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(fp, sep, engine='python', encoding = encoding)\n",
    "            df = cleandf(df)\n",
    "\n",
    "            # Convert inferred column types to pandas dtypes\n",
    "            column_types = [convert_to_dtype(inferred_type) for inferred_type in inferred_column_types]\n",
    "\n",
    "            # Set the column types of the DataFrame with preprocessing for non-numeric values\n",
    "            for col_name, dtype in zip(df.columns, column_types):\n",
    "                if dtype in ('int64', 'float64'):\n",
    "                    # Handle non-numeric values by replacing them with NaN\n",
    "                    df[col_name] = pd.to_numeric(df[col_name], errors='coerce')\n",
    "                    df[col_name].fillna(0, inplace=True)  # Replace NaN with 0\n",
    "                    df[col_name] = df[col_name].astype('float64')  # Convert to nullable float type\n",
    "                else:\n",
    "                    df[col_name] = df[col_name].astype(dtype)\n",
    "\n",
    "            # Set the column types of the DataFrame\n",
    "            df = df.astype(dict(zip(df.columns, column_types)))\n",
    "\n",
    "            # Create a fixed-length tensor of metadata\n",
    "            metadata_tensor = []\n",
    "            for col_name in df.columns:\n",
    "                column = df[col_name]\n",
    "                metadata = extract_metadata(column)\n",
    "                metadata_tensor.extend(metadata)\n",
    "\n",
    "            # Convert the metadata list to a PyTorch tensor\n",
    "            metadata_tensors.append(torch.tensor(metadata_tensor, dtype=torch.float))\n",
    "\n",
    "        # Pad each metadata tensor to match the maximum length\n",
    "        padded_metadata_tensors = []\n",
    "        for metadata_tensor in metadata_tensors:\n",
    "            padding_length = max_metadata_length - metadata_tensor.shape[0]\n",
    "            if padding_length < 0:\n",
    "                padded_metadata_tensor = metadata_tensor[:max_metadata_length]\n",
    "            else:\n",
    "                padded_metadata_tensor = torch.cat((metadata_tensor, torch.zeros(padding_length)))\n",
    "            padded_metadata_tensors.append(padded_metadata_tensor)\n",
    "\n",
    "        # Convert the list of padded metadata tensors to a single tensor\n",
    "        final_metadata_tensor = torch.stack(padded_metadata_tensors)\n",
    "\n",
    "        # Concatenate the feature tensor and final metadata tensor along the second dimension\n",
    "        X = torch.cat((X, final_metadata_tensor), dim=1)\n",
    "        \n",
    "    y = torch.tensor(numerical_community_labels, dtype=torch.long)\n",
    "    #print(f'X.shape:{X.shape}')\n",
    "    #print(f'y.shape:{y.shape}')\n",
    "\n",
    "    # Define parameters\n",
    "    #in_feats = len(feature_vocab)  # Number of unique features\n",
    "    in_feats = X.shape[1]\n",
    "    num_classes = len(community_labels)  # Number of communities\n",
    "    #print(f'in_feats:{in_feats}, num_classes:{num_classes}')\n",
    "\n",
    "    # Initialize the model\n",
    "    model = CommunityClassifier(in_feats, num_classes)\n",
    "    \n",
    "    return model, community_labels, feature_vocab, X, y\n",
    "\n",
    "def train_kfold(community_labels, X, y, random_state=42):\n",
    "    in_feats = X.shape[1]\n",
    "    num_classes = len(community_labels)\n",
    "    \n",
    "    # Create a StratifiedKFold instance with the desired number of folds\n",
    "    n_splits = 5  # Number of folds\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Lists to store evaluation metrics for each fold\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Perform Stratified K-Fold cross-validation\n",
    "    for fold, (train_idx, test_idx) in enumerate(stratified_kfold.split(X, y)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Initialize and train the model\n",
    "        # Convert numerical community labels to string class names\n",
    "        class_names = [str(label) for label in community_labels]\n",
    "        #print(f'len(class_names): {len(class_names)}')\n",
    "\n",
    "        # Define model, loss function, and optimizer\n",
    "        model = CommunityClassifier(in_feats, num_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(1000):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_train)\n",
    "            loss = criterion(output, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_output = model(X_test)\n",
    "            predicted_labels = torch.argmax(test_output, dim=1)\n",
    "\n",
    "        # Calculate evaluation metrics for the fold\n",
    "        accuracy = accuracy_score(y_test, predicted_labels)\n",
    "        precision = precision_score(y_test, predicted_labels, average='weighted')\n",
    "        recall = recall_score(y_test, predicted_labels, average='weighted')\n",
    "        f1 = f1_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        print(f\"Fold {fold+1} - Accuracy: {accuracy:.5f}, Precision: {precision:.5f}, Recall: {recall:.5f}, F1-Score: {f1:.5f}\")\n",
    "\n",
    "        # Calculate the average metrics across all folds\n",
    "        avg_accuracy = sum(accuracy_scores) / n_splits\n",
    "        avg_precision = sum(precision_scores) / n_splits\n",
    "        avg_recall = sum(recall_scores) / n_splits\n",
    "        avg_f1 = sum(f1_scores) / n_splits\n",
    "\n",
    "        print(\"\\nAverage Metrics Across Folds:\")\n",
    "        print(f\"Average Accuracy: {avg_accuracy:.5f}\")\n",
    "        print(f\"Average Precision: {avg_precision:.5f}\")\n",
    "        print(f\"Average Recall: {avg_recall:.5f}\")\n",
    "        print(f\"Average F1-Score: {avg_f1:.5f}\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "def train_shuffle(community_labels, X, y, test_size=0.3, random_state=42):\n",
    "    in_feats = X.shape[1]\n",
    "    num_classes = len(community_labels)\n",
    "    \n",
    "    # Create an instance of StratifiedShuffleSplit\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Convert numerical community labels to string class names\n",
    "    class_names = [str(label) for label in community_labels]\n",
    "    #print(f'len(class_names): {len(class_names)}')\n",
    "\n",
    "    # Define model, loss function, and optimizer\n",
    "    model = CommunityClassifier(in_feats, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1000):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the trained model on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output = model(X_test)\n",
    "        predicted_labels = torch.argmax(test_output, dim=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predicted_labels)\n",
    "    precision = precision_score(y_test, predicted_labels, average='weighted')\n",
    "    recall = recall_score(y_test, predicted_labels, average='weighted')\n",
    "    f1 = f1_score(y_test, predicted_labels, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "    class_report = classification_report(y_test, predicted_labels, target_names=class_names)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "    \n",
    "    # Compute ROC curve and ROC AUC for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test == i, test_output[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Print ROC AUC for each class\n",
    "    for i in range(num_classes):\n",
    "        print(f\"ROC AUC for Community {class_names[i]}: {roc_auc[i]:.4f}\")\n",
    "        \n",
    "        \n",
    "    # Print confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Display confusion matrix as a heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.set(font_scale=1)\n",
    "    ConfusionMatrixDisplay(conf_matrix, display_labels=class_names).plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot ROC Curves for each class\n",
    "    # Plot ROC Curves for each class\n",
    "    class_labels = [f\"ROC curve (Community {class_names[i]})\" for i in range(num_classes)]\n",
    "\n",
    "    # Create a figure with two subplots side by side\n",
    "    fig, (ax_data, ax_legend) = plt.subplots(1, 2, figsize=(8, 5), gridspec_kw={'width_ratios': [4, 1]})\n",
    "\n",
    "    # Plot ROC Curves for each class on the ax_data\n",
    "    legend_handles = []\n",
    "    for i in range(num_classes):\n",
    "        ax_data.plot(fpr[i], tpr[i])\n",
    "        # Create custom legend handles for the ROC curves\n",
    "        legend_handles.append(Line2D([0], [0], color='C{}'.format(i), lw=2))\n",
    "\n",
    "    ax_data.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    ax_data.set_xlabel('False Positive Rate')\n",
    "    ax_data.set_ylabel('True Positive Rate')\n",
    "    ax_data.set_title('ROC Curves for Each Community')\n",
    "\n",
    "    # Position the legend in the right subplot\n",
    "    ax_legend.legend(handles=legend_handles, labels=class_labels, bbox_to_anchor=(1, 1), loc='upper right')\n",
    "    ax_legend.axis(\"off\")\n",
    "\n",
    "    # Add a border around the entire figure\n",
    "    border_color = 'grey'  # Color of the border\n",
    "    border_width = 0.25  # Width of the border\n",
    "    fig.patch.set_edgecolor(border_color)\n",
    "    fig.patch.set_linewidth(border_width)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
